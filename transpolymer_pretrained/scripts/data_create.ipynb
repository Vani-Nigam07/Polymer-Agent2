{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5f249f",
   "metadata": {},
   "source": [
    "# Data Preparation for TransPolymer Finetuning\n",
    "\n",
    "This notebook handles data loading, validation, and splitting for finetuning the TransPolymer model.\n",
    "\n",
    "**Workflow:**\n",
    "1. Load input CSV file\n",
    "2. Inspect data types, rows, and data points\n",
    "3. Validate data quality\n",
    "4. Split into train/validation/test sets\n",
    "5. Save to data folder with clear names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c103328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Set paths\n",
    "DATA_FOLDER = Path(\"../data\")\n",
    "DATA_FOLDER.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Data folder: {DATA_FOLDER.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b529e66b",
   "metadata": {},
   "source": [
    "## Step 1: Load Input File\n",
    "\n",
    "Enter your input CSV file path below. The CSV should contain at least two columns:\n",
    "- Column 1: Polymer SMILES strings\n",
    "- Column 2: Target property values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0a50b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: Modify these values for your dataset\n",
    "INPUT_FILE = \"../data/OPV.csv\"  # Change this to your input file path\n",
    "DATASET_NAME = \"OPV\"  # Name for output files (e.g., OPV, PE_I, etc.)\n",
    "\n",
    "# Load the CSV file\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    print(f\"✓ Successfully loaded: {INPUT_FILE}\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"✗ Error: File not found at {INPUT_FILE}\")\n",
    "    print(f\"  Available files in data folder:\")\n",
    "    for f in DATA_FOLDER.glob(\"*.csv\"):\n",
    "        print(f\"    - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc925c5",
   "metadata": {},
   "source": [
    "## Step 2: Data Inspection & Validation\n",
    "\n",
    "Examine the data types, number of rows, and data statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33da9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n1. Dataset Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "print(f\"\\n2. Column Names and Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(f\"\\n3. First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\n4. Data Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(f\"\\n5. Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(f\"\\n6. Column Information:\")\n",
    "for col in df.columns:\n",
    "    print(f\"\\n   Column: '{col}'\")\n",
    "    print(f\"   - Data type: {df[col].dtype}\")\n",
    "    print(f\"   - Non-null count: {df[col].notna().sum()} / {len(df)}\")\n",
    "    print(f\"   - Unique values: {df[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4b3734",
   "metadata": {},
   "source": [
    "## Step 3: Data Cleaning & Validation\n",
    "\n",
    "Check for missing values, remove invalid entries, and validate data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for processing\n",
    "df_clean = df.copy()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA CLEANING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for missing values\n",
    "initial_rows = len(df_clean)\n",
    "print(f\"\\n1. Initial number of rows: {initial_rows}\")\n",
    "\n",
    "missing_info = df_clean.isnull().sum()\n",
    "if missing_info.sum() > 0:\n",
    "    print(f\"\\n2. Removing rows with missing values:\")\n",
    "    print(missing_info[missing_info > 0])\n",
    "    df_clean = df_clean.dropna()\n",
    "    print(f\"   Rows removed: {initial_rows - len(df_clean)}\")\n",
    "    print(f\"   Remaining rows: {len(df_clean)}\")\n",
    "else:\n",
    "    print(f\"\\n2. No missing values found ✓\")\n",
    "\n",
    "# Validate that we have at least 2 columns\n",
    "if len(df_clean.columns) < 2:\n",
    "    raise ValueError(f\"Expected at least 2 columns (SMILES, target), but got {len(df_clean.columns)}\")\n",
    "\n",
    "# Assume first column is SMILES and second is target property\n",
    "smiles_col = df_clean.columns[0]\n",
    "target_col = df_clean.columns[1]\n",
    "\n",
    "print(f\"\\n3. Column mapping:\")\n",
    "print(f\"   SMILES column: '{smiles_col}'\")\n",
    "print(f\"   Target property column: '{target_col}'\")\n",
    "\n",
    "# Check for duplicate SMILES\n",
    "duplicates = df_clean[smiles_col].duplicated().sum()\n",
    "print(f\"\\n4. Duplicate SMILES: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(f\"   Keeping first occurrence of duplicates...\")\n",
    "    df_clean = df_clean.drop_duplicates(subset=[smiles_col], keep='first')\n",
    "    print(f\"   Remaining rows: {len(df_clean)}\")\n",
    "\n",
    "# Validate target property is numeric\n",
    "try:\n",
    "    df_clean[target_col] = pd.to_numeric(df_clean[target_col])\n",
    "    print(f\"\\n5. Target property successfully converted to numeric ✓\")\n",
    "except ValueError as e:\n",
    "    print(f\"\\n5. Error: Could not convert target property to numeric\")\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "# Check for extreme values\n",
    "print(f\"\\n6. Target property range:\")\n",
    "print(f\"   Min: {df_clean[target_col].min()}\")\n",
    "print(f\"   Max: {df_clean[target_col].max()}\")\n",
    "print(f\"   Mean: {df_clean[target_col].mean():.4f}\")\n",
    "print(f\"   Std Dev: {df_clean[target_col].std():.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINAL DATASET: {len(df_clean)} rows × {len(df_clean.columns)} columns\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af18a779",
   "metadata": {},
   "source": [
    "## Step 4: Train/Validation/Test Split\n",
    "\n",
    "Split the data for finetuning with validation set:\n",
    "- **Training set**: 70% - used for training the model\n",
    "- **Validation set**: 15% - used for hyperparameter tuning and early stopping\n",
    "- **Test set**: 15% - used for final evaluation\n",
    "\n",
    "The validation set is important for preventing overfitting during finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44723601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split configuration\n",
    "RANDOM_STATE = 42  # For reproducibility\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA SPLITTING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nSplit Ratios:\")\n",
    "print(f\"  Train: {TRAIN_RATIO*100:.0f}%\")\n",
    "print(f\"  Validation: {VAL_RATIO*100:.0f}%\")\n",
    "print(f\"  Test: {TEST_RATIO*100:.0f}%\")\n",
    "print(f\"  Random state: {RANDOM_STATE}\")\n",
    "\n",
    "# First split: separate test set (15%)\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df_clean,\n",
    "    test_size=TEST_RATIO,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Second split: separate validation from training (15% of remaining 85%)\n",
    "# val_ratio = 0.15 / 0.85 ≈ 0.176 to get 15% of total\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df,\n",
    "    test_size=VAL_RATIO / (1 - TEST_RATIO),\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\nSplit Results:\")\n",
    "print(f\"  Training set: {len(train_df)} samples ({len(train_df)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"  Validation set: {len(val_df)} samples ({len(val_df)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"  Test set: {len(test_df)} samples ({len(test_df)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"  Total: {len(train_df) + len(val_df) + len(test_df)} samples\")\n",
    "\n",
    "# Display statistics for each split\n",
    "print(f\"\\nTarget Property Statistics by Split:\")\n",
    "print(f\"\\n  Training Set ({target_col}):\")\n",
    "print(f\"    Mean: {train_df[target_col].mean():.4f}\")\n",
    "print(f\"    Std Dev: {train_df[target_col].std():.4f}\")\n",
    "print(f\"    Range: [{train_df[target_col].min():.4f}, {train_df[target_col].max():.4f}]\")\n",
    "\n",
    "print(f\"\\n  Validation Set ({target_col}):\")\n",
    "print(f\"    Mean: {val_df[target_col].mean():.4f}\")\n",
    "print(f\"    Std Dev: {val_df[target_col].std():.4f}\")\n",
    "print(f\"    Range: [{val_df[target_col].min():.4f}, {val_df[target_col].max():.4f}]\")\n",
    "\n",
    "print(f\"\\n  Test Set ({target_col}):\")\n",
    "print(f\"    Mean: {test_df[target_col].mean():.4f}\")\n",
    "print(f\"    Std Dev: {test_df[target_col].std():.4f}\")\n",
    "print(f\"    Range: [{test_df[target_col].min():.4f}, {test_df[target_col].max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eea381",
   "metadata": {},
   "source": [
    "## Step 5: Save Processed Data\n",
    "\n",
    "Save the train, validation, and test sets with clear names to the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c35499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SAVING PROCESSED DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define output file names with clear naming convention\n",
    "# Format: {DATASET_NAME}_{split_type}_{purpose}.csv\n",
    "# Example: OPV_train.csv, OPV_val.csv, OPV_test.csv\n",
    "\n",
    "train_file = DATA_FOLDER / f\"{DATASET_NAME}_train.csv\"\n",
    "val_file = DATA_FOLDER / f\"{DATASET_NAME}_val.csv\"\n",
    "test_file = DATA_FOLDER / f\"{DATASET_NAME}_test.csv\"\n",
    "\n",
    "# Save files\n",
    "try:\n",
    "    train_df.to_csv(train_file, index=False)\n",
    "    print(f\"\\n✓ Saved training set: {train_file.name}\")\n",
    "    print(f\"  Size: {len(train_df)} samples\")\n",
    "    \n",
    "    val_df.to_csv(val_file, index=False)\n",
    "    print(f\"\\n✓ Saved validation set: {val_file.name}\")\n",
    "    print(f\"  Size: {len(val_df)} samples\")\n",
    "    \n",
    "    test_df.to_csv(test_file, index=False)\n",
    "    print(f\"\\n✓ Saved test set: {test_file.name}\")\n",
    "    print(f\"  Size: {len(test_df)} samples\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"All files saved successfully to: {DATA_FOLDER.absolute()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error saving files: {e}\")\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nFile Summary:\")\n",
    "print(f\"  Train: {train_file.name} ({len(train_df)} rows)\")\n",
    "print(f\"  Val:   {val_file.name} ({len(val_df)} rows)\")\n",
    "print(f\"  Test:  {test_file.name} ({len(test_df)} rows)\")\n",
    "print(f\"\\nColumn structure (all files):\")\n",
    "print(f\"  Column 1: {smiles_col} (Polymer SMILES)\")\n",
    "print(f\"  Column 2: {target_col} (Target property)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fe4cbf",
   "metadata": {},
   "source": [
    "## Step 6: Verification\n",
    "\n",
    "Verify the saved files by reloading and checking their contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fce6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Reload and verify each file\n",
    "for file_path, set_name in [(train_file, \"Training\"), (val_file, \"Validation\"), (test_file, \"Test\")]:\n",
    "    if file_path.exists():\n",
    "        df_verify = pd.read_csv(file_path)\n",
    "        print(f\"\\n✓ {set_name} Set - {file_path.name}\")\n",
    "        print(f\"  Rows: {len(df_verify)}\")\n",
    "        print(f\"  Columns: {list(df_verify.columns)}\")\n",
    "        print(f\"  Data types:\\n{df_verify.dtypes}\")\n",
    "        print(f\"  Sample row:\")\n",
    "        print(f\"    {df_verify.iloc[0].to_dict()}\")\n",
    "    else:\n",
    "        print(f\"\\n✗ File not found: {file_path.name}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Data preparation complete! ✓\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce98b78d",
   "metadata": {},
   "source": [
    "## Step 7: Summary & Notes\n",
    "\n",
    "### About the Splits:\n",
    "- **Training Set (70%)**: Used to train the finetuned model\n",
    "- **Validation Set (15%)**: Used for:\n",
    "  - Early stopping during training\n",
    "  - Hyperparameter tuning\n",
    "  - Model selection\n",
    "  - Monitoring overfitting\n",
    "- **Test Set (15%)**: Used for final evaluation (held out, not touched during training)\n",
    "\n",
    "### Why Validation Set is Important for Finetuning:\n",
    "1. **Prevents overfitting**: Monitor performance on unseen data during training\n",
    "2. **Early stopping**: Stop training when validation loss stops improving\n",
    "3. **Hyperparameter tuning**: Test different learning rates, batch sizes, etc.\n",
    "4. **Final evaluation**: Test set remains completely untouched\n",
    "\n",
    "### Output Files Generated:\n",
    "- `{DATASET_NAME}_train.csv` - Training data\n",
    "- `{DATASET_NAME}_val.csv` - Validation data\n",
    "- `{DATASET_NAME}_test.csv` - Test data\n",
    "\n",
    "All files contain the same columns (SMILES, target property) and are ready for finetuning!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
